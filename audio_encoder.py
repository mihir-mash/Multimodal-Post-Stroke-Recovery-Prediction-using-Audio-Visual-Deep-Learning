# -*- coding: utf-8 -*-
"""Audio_Encoder.ipynb

Automatically generated by Colab.

Original file is located at

#Word2Vec2Processor
"""

from google.colab import drive
drive.mount('/content/drive')

# Install necessary libraries
!pip install -q transformers datasets torchaudio librosa
from google.colab import drive
import os
import librosa
import numpy as np
import torch
from transformers import Wav2Vec2Processor, Wav2Vec2Model

# Mount Drive
drive.mount('/content/drive')
DATA_PATH = "path_to_dataset"

def extract_motor_speech_features(audio_file):
    # Load audio (16kHz is standard for Wav2Vec2)
    y, sr = librosa.load(audio_file, sr=16000)

    # 1. Extract MFCCs (Mel-frequency cepstral coefficients)
    # These represent the 'shape' of the vocal tract during speech
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    mfcc_mean = np.mean(mfccs, axis=1)

    # 2. Extract Speech Rate (Syllables per second for DDK tasks)
    # Simple estimate using onset strength peaks
    onset_env = librosa.onset.onset_strength(y=y, sr=sr)
    peaks = librosa.util.peak_pick(onset_env, pre_max=7, post_max=7, pre_avg=7, post_avg=7, delta=0.5, wait=7)
    duration = librosa.get_duration(y=y, sr=sr)
    speech_rate = len(peaks) / duration if duration > 0 else 0

    # 3. Intensity Decay (Fatigue)
    rms = librosa.feature.rms(y=y)[0]
    intensity_decay = rms[0] - rms[-1] # Simple start-vs-end difference

    return {
        "mfccs": mfcc_mean,
        "speech_rate": speech_rate,
        "intensity_decay": intensity_decay
    }

# Load Pre-trained Model
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")

def get_wav2vec_embeddings(audio_file):
    y, sr = librosa.load(audio_file, sr=16000)
    input_values = processor(y, return_tensors="pt", sampling_rate=16000).input_values

    with torch.no_grad():
        outputs = model(input_values)
        # Extract the 'hidden_states' or context representations
        embeddings = torch.mean(outputs.last_hidden_state, dim=1)

    return embeddings.squeeze().numpy()

import os
import zipfile
import glob
import pandas as pd

# Path to your Drive folder
DRIVE_PATH = "path_to_dataset"
EXTRACT_DIR = "audio_path"

def prepare_neuroface_audio_dataset(root_path):
    os.makedirs(EXTRACT_DIR, exist_ok=True)
    dataset_records = []

    # EXACT names from Toronto NeuroFace README
    groups = ['Healthy controls', 'Stroke', 'ALS']
    label_map = {'Healthy controls': 0, 'Stroke': 1, 'ALS': 2}

    print(f"Checking root path: {root_path}")

    for group in groups:
        group_path = os.path.join(root_path, group)

        # Check if the group folder exists
        if not os.path.exists(group_path):
            print(f"Warning: Folder '{group}' not found at {group_path}. Skipping.")
            continue

        # Recursive search for Audios.zip
        # TNF structure: Group / SubjectID / Audios.zip
        zip_files = glob.glob(f"{group_path}/**/Audios.zip", recursive=True)
        print(f"Found {len(zip_files)} zip files in {group}")

        for zip_path in zip_files:
            # Extract subject ID (3rd item from the end of the path)
            parts = zip_path.split('/')
            subject_id = parts[-2]

            extract_to = os.path.join(EXTRACT_DIR, group, subject_id)
            os.makedirs(extract_to, exist_ok=True)

            try:
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_to)

                # Find the .wav files inside the extracted folder
                wav_files = glob.glob(f"{extract_to}/**/*.wav", recursive=True)
                for wav in wav_files:
                    dataset_records.append({
                        "file_path": wav,
                        "label": label_map[group],
                        "group": group,
                        "subject": subject_id,
                        "task": os.path.basename(wav).split('_')[-2] # e.g., 'BBP' or 'PA'
                    })
            except Exception as e:
                print(f"Error unzipping {zip_path}: {e}")

    return pd.DataFrame(dataset_records)

# Run extraction
audio_df = prepare_neuroface_audio_dataset(DRIVE_PATH)

if len(audio_df) > 0:
    print(f"Successfully found and extracted {len(audio_df)} audio files.")
    print(audio_df.head())
else:
    print("Zero files found. Please run: !ls 'path_to_dataset' to verify folder names.")

import torch
from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset, Audio

# Ensure transformers is updated to avoid potential version-related issues with TrainingArguments
!pip install --upgrade transformers

# 1. Convert DataFrame to HuggingFace Dataset
ds = Dataset.from_pandas(audio_df).cast_column("file_path", Audio(sampling_rate=16_000))

# 2. Preprocessing
model_id = "facebook/wav2vec2-base-960h"
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["file_path"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16_000,
        max_length=16000*5, # Truncate to 5 seconds
        truncation=True
    )
    return inputs

encoded_ds = ds.map(preprocess_function, remove_columns=["file_path"], batched=True)

# Split the dataset into training and evaluation sets
train_test_split = encoded_ds.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# 3. Model Setup
model = AutoModelForAudioClassification.from_pretrained(
    model_id,
    num_labels=3,
    label2id={'Healthy': 0, 'Stroke': 1, 'ALS': 2},
    id2label={0: 'Healthy', 1: 'Stroke', 2: 'ALS'}
)

# Freeze the feature encoder to save memory and prevent overfitting
model.freeze_feature_encoder()

# 4. Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch", # Changed from evaluation_strategy to eval_strategy
    save_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    num_train_epochs=10,
    logging_steps=10,
)

# 5. Initialize Trainer
# Define a DataCollator for padding
data_collator = DataCollatorWithPadding(tokenizer=feature_extractor, padding=True)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset, # Pass the training dataset
    eval_dataset=eval_dataset,   # Pass the evaluation dataset
    data_collator=data_collator, # Pass the data collator here
)

# 6. Train
trainer.train()

# Check performance on the test set
eval_results = trainer.evaluate()
print(eval_results)

import torch
from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset, Audio


# 1. Convert DataFrame to HuggingFace Dataset
ds = Dataset.from_pandas(audio_df).cast_column("file_path", Audio(sampling_rate=16_000))

# 2. Preprocessing
model_id = "facebook/wav2vec2-base-960h"
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["file_path"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16_000,
        max_length=16000*5, # Truncate to 5 seconds
        truncation=True
    )
    return inputs

encoded_ds = ds.map(preprocess_function, remove_columns=["file_path"], batched=True)

# Split the dataset into training and evaluation sets
train_test_split = encoded_ds.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# 1. Enhanced Model Setup
model_id = "facebook/wav2vec2-base-960h"
model = AutoModelForAudioClassification.from_pretrained(
    model_id,
    num_labels=3,
    label2id={'Healthy': 0, 'Stroke': 1, 'ALS': 2},
    id2label={0: 'Healthy', 1: 'Stroke', 2: 'ALS'}
)

# CRITICAL: Fix for 'nan' loss and stability on small data
model.config.ctc_zero_infinity = True
model.config.layerdrop = 0.1  # Helps prevent overfitting
model.freeze_feature_encoder()

# 2. Optimized Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-5,            # Lower LR for stability
    per_device_train_batch_size=4, # Smaller batch for small dataset
    num_train_epochs=15,           # Slightly more epochs to see convergence
    weight_decay=0.01,             # Regularization
    fp16=True,                     # Mixed precision to save memory
    logging_steps=5,
    load_best_model_at_end=True
)

# 3. Trainer with Evaluation
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=DataCollatorWithPadding(tokenizer=feature_extractor, padding=True),
)

trainer.train()

import torch
from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset, Audio


# 1. Convert DataFrame to HuggingFace Dataset
ds = Dataset.from_pandas(audio_df).cast_column("file_path", Audio(sampling_rate=16_000))

# 2. Preprocessing
model_id = "facebook/wav2vec2-base-960h"
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["file_path"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16_000,
        max_length=16000*5, # Truncate to 5 seconds
        truncation=True
    )
    return inputs

encoded_ds = ds.map(preprocess_function, remove_columns=["file_path"], batched=True)

# Split the dataset into training and evaluation sets
train_test_split = encoded_ds.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# 1. Enhanced Model Setup
model_id = "facebook/wav2vec2-base-960h"
model = AutoModelForAudioClassification.from_pretrained(
    model_id,
    num_labels=3,
    label2id={'Healthy': 0, 'Stroke': 1, 'ALS': 2},
    id2label={0: 'Healthy', 1: 'Stroke', 2: 'ALS'}
)

# CRITICAL: Fix for 'nan' loss and stability on small data
model.config.ctc_zero_infinity = True
model.config.layerdrop = 0.1  # Helps prevent overfitting
model.freeze_feature_encoder()

# 2. Optimized Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-5,            # Lower LR for stability
    per_device_train_batch_size=4, # Smaller batch for small dataset
    num_train_epochs=15,           # Slightly more epochs to see convergence
    weight_decay=0.01,             # Regularization
    fp16=True,                     # Mixed precision to save memory
    logging_steps=5,
    load_best_model_at_end=True
)

# 3. Trainer with Evaluation
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=DataCollatorWithPadding(tokenizer=feature_extractor, padding=True),
)

trainer.train()

!pip install librosa soundfile

"""#Audio Encoder"""

from google.colab import drive
drive.mount('/content/drive')
import os
import numpy as np
import librosa
import torch
from tqdm import tqdm

import zipfile
import os

zip_path = "path_to_dataset"
extract_path = "path_to_dataset"

# Create folder if not exists
os.makedirs(extract_path, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Unzipped successfully!")

import numpy as np
import librosa
import torch
from tqdm import tqdm
def preprocess_audio(file_path, target_sr=16000, duration=4):
    y, sr = librosa.load(file_path, sr=target_sr)

    # Trim silence
    y, _ = librosa.effects.trim(y)

    # Normalize safely
    if np.max(np.abs(y)) > 0:
        y = y / np.max(np.abs(y))

    # Pad or truncate
    max_len = target_sr * duration
    if len(y) < max_len:
        y = np.pad(y, (0, max_len - len(y)))
    else:
        y = y[:max_len]

    return y, target_sr

def extract_audio_features(y, sr):

    features = []

    # Energy features
    features.append(np.mean(y**2))  # energy
    features.append(np.std(y))      # amplitude variation

    # Speech rate (onset count)
    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
    duration = len(y) / sr
    speech_rate = len(onset_frames) / duration
    features.append(speech_rate)

    # Interval variability
    if len(onset_frames) > 1:
        intervals = np.diff(onset_frames)
        features.append(np.std(intervals))
    else:
        features.append(0.0)

    # MFCCs
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    features.extend(np.mean(mfcc, axis=1))
    features.extend(np.std(mfcc, axis=1))

    return np.array(features)

audio_root = "path_to_dataset"

audio_data = []
audio_subjects = []
audio_tasks = []

for file in os.listdir(audio_root):
    if file.lower().endswith(".wav"):

        file_path = os.path.join(audio_root, file)

        name = file.replace(".wav","").replace(".WAV","")
        parts = name.split("_")

        subject = parts[0]
        task = "_".join(parts[2:])

        y, sr = preprocess_audio(file_path)
        feats = extract_audio_features(y, sr)

        audio_data.append(feats)
        audio_subjects.append(subject)
        audio_tasks.append(task)

audio_data = np.array(audio_data)

print("Feature shape:", audio_data.shape)

audio_data = np.array(audio_data)

audio_dataset = {
    "features": torch.tensor(audio_data, dtype=torch.float32),
    "subjects": audio_subjects,
    "tasks": audio_tasks
}

save_path = "saving_path"

torch.save(audio_dataset, save_path)

print("Saved at:", save_path)
print("Feature shape:", audio_data.shape)

audio_root = "path_to_dataset"

print(os.listdir(audio_root))

for root, dirs, files in os.walk(audio_root):
    print("Root:", root)
    print("Files:", files)
    print("-"*40)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

audio_data_scaled = scaler.fit_transform(audio_data)

audio_data_scaled = torch.tensor(audio_data_scaled, dtype=torch.float32)

print("Scaled shape:", audio_data_scaled.shape)

import joblib

scaler_path = "path_to_scaler"
joblib.dump(scaler, scaler_path)

print("Scaler saved.")

audio_dataset = {
    "features": audio_data_scaled,
    "subjects": audio_subjects,
    "tasks": audio_tasks
}

save_path = "path_to trained model-.pt"
torch.save(audio_dataset, save_path)

print("Saved scaled dataset.")

import torch.nn as nn

class AudioEncoder(nn.Module):
    def __init__(self, input_dim=30, embed_dim=32):
        super(AudioEncoder, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, embed_dim),
            nn.ReLU()
        )

    def forward(self, x):
        return self.network(x)

model = AudioEncoder()

dummy_output = model(audio_data_scaled)

print("Audio embedding shape:", dummy_output.shape)

"""#Improving

"""

!pip install praat-parselmouth

import parselmouth
from parselmouth.praat import call

def extract_advanced_audio_features(y, sr, file_path):

    features = []

    # ---------------- Energy ----------------
    features.append(np.mean(y**2))
    features.append(np.std(y))

    # ---------------- Speech Rate ----------------
    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
    duration = len(y) / sr
    speech_rate = len(onset_frames) / duration
    features.append(speech_rate)

    if len(onset_frames) > 1:
        intervals = np.diff(onset_frames)
        features.append(np.std(intervals))
    else:
        features.append(0.0)

    # ---------------- MFCC ----------------
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    features.extend(np.mean(mfcc, axis=1))
    features.extend(np.std(mfcc, axis=1))

    # ---------------- Jitter & Shimmer ----------------
    try:
        sound = parselmouth.Sound(file_path)
        point_process = call(sound, "To PointProcess (periodic, cc)", 75, 500)

        jitter = call(point_process, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3)
        shimmer = call([sound, point_process], "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)

    except:
        jitter = 0.0
        shimmer = 0.0

    features.append(jitter)
    features.append(shimmer)

    # ---------------- Pause Ratio (for BBP) ----------------
    # Voice Activity Detection approximation
    intervals = librosa.effects.split(y, top_db=30)
    speech_duration = sum((end - start) for start, end in intervals)
    pause_ratio = 1 - (speech_duration / len(y))
    features.append(pause_ratio)

    return np.array(features)

