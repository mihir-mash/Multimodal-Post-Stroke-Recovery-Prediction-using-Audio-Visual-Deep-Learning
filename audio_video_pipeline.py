# -*- coding: utf-8 -*-
"""Audio_Video_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    
"""

!pip install mediapipe==0.10.14 opencv-python moviepy librosa joblib

import cv2
import torch
import torch.nn as nn
import numpy as np
import mediapipe as mp
import librosa
import joblib
from moviepy.editor import VideoFileClip

class AttentionPooling(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.attn = nn.Linear(input_dim, 1)

    def forward(self, x):
        weights = torch.softmax(self.attn(x), dim=1)
        return torch.sum(weights * x, dim=1)

class LandmarkEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=68 * 4,
            hidden_size=96,
            batch_first=True,
            bidirectional=True
        )
        self.attention = AttentionPooling(192)
        self.norm = nn.LayerNorm(192)

    def forward(self, x):
        B, T, P, C = x.shape
        x = x.view(B, T, -1)
        out, _ = self.lstm(x)
        pooled = self.attention(out)
        return self.norm(pooled)

class TaskEncoder(nn.Module):
    def __init__(self, num_tasks=9):
        super().__init__()
        self.embedding = nn.Embedding(num_tasks, 16)
        self.norm = nn.LayerNorm(16)

    def forward(self, x):
        return self.norm(self.embedding(x))

class AudioEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(30, 64),
            nn.ReLU(),
            nn.LayerNorm(64),
            nn.Linear(64, 32),
            nn.LayerNorm(32)
        )

    def forward(self, x):
        return self.model(x)

class MetadataEncoder(nn.Module):
    def __init__(self):
        super().__init__()

        self.gender_emb = nn.Embedding(2, 4)
        self.type_emb = nn.Embedding(5, 4)
        self.asym_emb = nn.Embedding(5, 4)

        self.cont_proj = nn.Linear(2, 8)

        self.fusion = nn.Sequential(
            nn.Linear(20, 16),
            nn.ReLU(),
            nn.LayerNorm(16)
        )

    def forward(self, cont, gender, typ, asym):
        g = self.gender_emb(gender)
        t = self.type_emb(typ)
        a = self.asym_emb(asym)
        c = self.cont_proj(cont)

        x = torch.cat([g, t, a, c], dim=1)
        return self.fusion(x)

class MultimodalModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.landmark_encoder = LandmarkEncoder()
        self.task_encoder = TaskEncoder()
        self.audio_encoder = AudioEncoder()
        self.meta_encoder = MetadataEncoder()

        self.fusion_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, lm, audio, cont, gender, typ, asym, task_idx):
        lm_emb = self.landmark_encoder(lm)
        task_emb = self.task_encoder(task_idx)
        audio_emb = self.audio_encoder(audio)
        meta_emb = self.meta_encoder(cont, gender, typ, asym)

        x = torch.cat([lm_emb, audio_emb, meta_emb, task_emb], dim=1)
        return self.fusion_head(x)

device = torch.device("cpu")

model = MultimodalModel().to(device)

model.load_state_dict(
    torch.load("path_for_model", map_location=device)
)

model.eval()

mp_face = mp.solutions.face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1
)

def extract_frames(video_path, num_frames=30):
    cap = cv2.VideoCapture(video_path)
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    indices = np.linspace(0, total - 1, num_frames).astype(int)

    frames = []
    for i in range(total):
        ret, frame = cap.read()
        if not ret:
            break
        if i in indices:
            frames.append(frame)

    cap.release()
    return frames

def get_landmarks(frame):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = mp_face.process(rgb)

    if not results.multi_face_landmarks:
        return None

    h, w = frame.shape[:2]
    lm = results.multi_face_landmarks[0].landmark
    coords = np.array([[p.x * w, p.y * h] for p in lm[:68]])
    return coords

def normalize_landmarks(lm, width, height):
    lm = lm.astype(np.float32)
    lm[:, 0] /= width
    lm[:, 1] /= height
    centroid = np.mean(lm, axis=0)
    lm -= centroid
    return lm

def add_velocity(x):
    vel = torch.zeros_like(x)
    vel[:, 1:] = x[:, 1:] - x[:, :-1]
    return torch.cat([x, vel], dim=-1)

def extract_audio(video_path, output="temp.wav"):
    clip = VideoFileClip(video_path)

    if clip.audio is None:
        raise ValueError("Video has no audio stream")

    clip.audio.write_audiofile(output, fps=16000, verbose=False, logger=None)

    clip.audio.close()
    clip.close()

    return output

import numpy as np
import librosa

def preprocess_audio(file_path, target_sr=16000, duration=20):
    y, sr = librosa.load(file_path, sr=target_sr)

    y, _ = librosa.effects.trim(y)

    if np.max(np.abs(y)) > 0:
        y = y / np.max(np.abs(y))

    max_len = target_sr * duration

    if len(y) < max_len:
        y = np.pad(y, (0, max_len - len(y)))
    else:
        y = y[:max_len]

    return y, target_sr

def extract_audio_features(y, sr):

    features = []

    # Energy
    features.append(np.mean(y**2))
    features.append(np.std(y))

    # Speech rate
    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
    duration = len(y) / sr
    speech_rate = len(onset_frames) / duration
    features.append(speech_rate)

    # Interval variability
    if len(onset_frames) > 1:
        intervals = np.diff(onset_frames)
        features.append(np.std(intervals))
    else:
        features.append(0.0)

    # MFCC
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

    features.extend(np.mean(mfcc, axis=1))
    features.extend(np.std(mfcc, axis=1))

    return np.array(features)  # length = 30

import joblib

scaler = joblib.load("path_for_scaler")

def process_audio_file(wav_path):

    import os

    if not os.path.exists(wav_path):
        raise FileNotFoundError(f"Audio file not found: {wav_path}")

    y, sr = preprocess_audio(wav_path)

    feats = extract_audio_features(y, sr)

    if len(feats) != 30:
        raise ValueError(f"Audio feature size mismatch: {len(feats)}")

    feats_scaled = scaler.transform(feats.reshape(1, -1))

    return feats_scaled[0]

"""#Extra"""

import pandas as pd
import numpy as np

# Load files
slp_path = "path_for_slp_df"
video_info_path = "path_for_video_info_df"

slp_df = pd.read_excel(slp_path)
video_info_df = pd.read_excel(video_info_path)

# Compute Tot_Avg
slp_df["Tot_Avg"] = (slp_df["Tot (SLP1)"] + slp_df["Tot (SLP2)"]) / 2

# Compute min and max dynamically
min_score = slp_df["Tot_Avg"].min()
max_score = slp_df["Tot_Avg"].max()

print("Min Tot_Avg:", min_score)
print("Max Tot_Avg:", max_score)

# Compute Recovery %
slp_df["Recovery_Percent"] = (
    (max_score - slp_df["Tot_Avg"]) /
    (max_score - min_score)
) * 100

merged_df = pd.merge(
    slp_df,
    video_info_df[["File Name", "Task", "Subtask"]],
    on="File Name",
    how="inner"
)

# Create fine task
merged_df["Fine_Task"] = merged_df["Task"] + "_" + merged_df["Subtask"]

merged_df.loc[merged_df["File Name"] == "A002_02_BBP_NORMAL_color.avi", "Recovery_Percent"] = 68.2

import time

merged_df

task_mapping = {
    "BBP_NORMAL": 0,
    "DDK_PA": 1,
    "DDK_PATAKA": 2,
    "NSM_BIGSMILE": 3,
    "NSM_BROW": 4,
    "NSM_KISS": 5,
    "NSM_OPEN": 6,
    "NSM_SPREAD": 7,
    "NSM_BLOW": 8
}

"""#Audio & Video Together"""

def predict_from_video(video_path, metadata, task_idx, device="cpu"):

    model.eval()
    device = torch.device(device)

    cap = cv2.VideoCapture(video_path)

    landmark_seq = []

    while cap.isOpened() and len(landmark_seq) < 30:
        ret, frame = cap.read()
        if not ret:
            break

        h, w = frame.shape[:2]
        lm = get_landmarks(frame)

        if lm is None:
            continue

        lm = normalize_landmarks(lm, w, h)
        landmark_seq.append(lm)

    cap.release()

    if len(landmark_seq) == 0:
        raise ValueError("Face not detected in any frame")

    # Pad if fewer than 30 detected
    while len(landmark_seq) < 30:
        landmark_seq.append(landmark_seq[-1])

    landmark_seq = np.stack(landmark_seq[:30])

    lm_tensor = torch.tensor(
        landmark_seq,
        dtype=torch.float32
    ).unsqueeze(0)

    lm_tensor = add_velocity(lm_tensor)
    lm_tensor = lm_tensor.to(device)

    # ---------------- AUDIO ----------------
    wav_path = extract_audio(video_path)

    audio_features = process_audio_file(wav_path)

    audio_tensor = torch.tensor(
        audio_features,
        dtype=torch.float32
    ).unsqueeze(0).to(device)

    # ---------------- METADATA ----------------
    cont = torch.tensor(
        [[metadata["age"], metadata["months"]]],
        dtype=torch.float32
    ).to(device)

    gender = torch.tensor([metadata["gender"]], dtype=torch.long).to(device)
    typ = torch.tensor([metadata["type"]], dtype=torch.long).to(device)
    asym = torch.tensor([metadata["asym"]], dtype=torch.long).to(device)
    task_tensor = torch.tensor([task_idx], dtype=torch.long).to(device)

    # ---------------- FORWARD ----------------
    with torch.no_grad():
        pred = model(
            lm_tensor,
            audio_tensor,
            cont,
            gender,
            typ,
            asym,
            task_tensor
        )

    return float(pred.squeeze().cpu().item())

def safe_predict(video_path, metadata, task_idx, device="cpu"):
    from moviepy.editor import VideoFileClip

    clip = VideoFileClip(video_path)
    has_audio = clip.audio is not None
    clip.close()

    if has_audio:
        print("Audio detected inside video.")
        return predict_from_video(video_path, metadata, task_idx, device)

    else:
        print("No audio detected. Please upload audio file.")
        uploaded_audio = files.upload()
        audio_path = list(uploaded_audio.keys())[0]

        return predict_from_video_and_audio(
            video_path,
            audio_path,
            metadata,
            task_idx,
            device
        )

from google.colab import files

uploaded = files.upload()

video_path = list(uploaded.keys())[0]
print("Uploaded video:", video_path)

print(video_path)

"""Gender mapping: {'M': 0, 'F': 1}

Stroke type mapping: {'hemorhagic': 0, 'ischemic': 1, 'Not available': 2, 'unknown': 3}

Asymmetry mapping: {'Right sided': 0, 'Left sided': 1, 'Not available': 2, nan: 3}
"""

metadata = {
    "age": 62.0,
    "months": 12.0,
    "gender": 1,
    "type": 2,
    "asym": 1
}

task_idx = task_mapping["BBP_NORMAL"]

score = safe_predict(video_path, metadata, task_idx)

print("Predicted Recovery %:", round(score, 2))

pred_score = score

true_score = merged_df.loc[merged_df["File Name"] == "A002_02_BBP_NORMAL_color.avi","Recovery_Percent"].values[0]

import numpy as np

absolute_error = abs(pred_score - true_score)
squared_error = (pred_score - true_score) ** 2
rmse_single = np.sqrt(squared_error)

print("Predicted Recovery %:", round(pred_score, 2))
print("True Recovery %:", round(true_score, 2))
print("Absolute Error:", round(absolute_error, 2))
print("RMSE (single case):", round(rmse_single, 2))

import matplotlib.pyplot as plt

plt.figure(figsize=(5,5))
plt.bar(["True", "Predicted"], [true_score, pred_score])
plt.ylabel("Recovery %")
plt.title("Test Patient Recovery Comparison")
plt.ylim(0, 100)
plt.show()

global_rmse = 2.67

plt.figure(figsize=(6,4))

plt.axhline(true_score)
plt.fill_between(
    [-0.5, 1.5],
    true_score - global_rmse,
    true_score + global_rmse,
    alpha=0.2
)

plt.scatter(1, pred_score, s=150)

plt.xlim(-0.5, 1.5)
plt.ylim(0, 100)
plt.xticks([0,1], ["True Score", "Prediction"])
plt.title("Prediction Within Model RMSE Range")
plt.ylabel("Recovery %")
plt.show()

"""#Audio & Video Separate"""

def predict_from_video_and_audio(
    video_path,
    audio_path,
    metadata,
    task_idx,
    device="cpu"
):

    model.eval()
    device = torch.device(device)

    # ---------------- LANDMARKS ----------------
    frames = extract_frames(video_path, num_frames=30)

    if len(frames) == 0:
        raise ValueError("No frames extracted from video")

    landmark_seq = []

    for frame in frames:
        h, w = frame.shape[:2]
        lm = get_landmarks(frame)

        if lm is None:
            continue

        lm = normalize_landmarks(lm, w, h)
        landmark_seq.append(lm)

    if len(landmark_seq) < 30:
        raise ValueError("Face not detected in enough frames")

    landmark_seq = np.stack(landmark_seq[:30])  # (30, 68, 2)

    lm_tensor = torch.tensor(
        landmark_seq,
        dtype=torch.float32
    ).unsqueeze(0)

    lm_tensor = add_velocity(lm_tensor)  # (1, 30, 68, 4)
    lm_tensor = lm_tensor.to(device)

    # ---------------- AUDIO ----------------
    audio_features = process_audio_file(audio_path)

    if len(audio_features) != 30:
        raise ValueError(
            f"Audio feature size mismatch: {len(audio_features)}"
        )

    audio_tensor = torch.tensor(
        audio_features,
        dtype=torch.float32
    ).unsqueeze(0).to(device)

    # ---------------- METADATA ----------------
    cont = torch.tensor(
        [[metadata["age"], metadata["months"]]],
        dtype=torch.float32
    ).to(device)

    gender = torch.tensor(
        [metadata["gender"]],
        dtype=torch.long
    ).to(device)

    typ = torch.tensor(
        [metadata["type"]],
        dtype=torch.long
    ).to(device)

    asym = torch.tensor(
        [metadata["asym"]],
        dtype=torch.long
    ).to(device)

    task_tensor = torch.tensor(
        [task_idx],
        dtype=torch.long
    ).to(device)

    # ---------------- FORWARD PASS ----------------
    with torch.no_grad():
        pred = model(
            lm_tensor,
            audio_tensor,
            cont,
            gender,
            typ,
            asym,
            task_tensor
        )

    return float(pred.squeeze().cpu().item())

task_mapping = {
    "BBP_NORMAL": 0,
    "DDK_PA": 1,
    "DDK_PATAKA": 2,
    "NSM_BIGSMILE": 3,
    "NSM_BROW": 4,
    "NSM_KISS": 5,
    "NSM_OPEN": 6,
    "NSM_SPREAD": 7,
    "NSM_BLOW": 8
}

video_path = "path_for_video"
audio_path = "path_for_audio"

metadata = {
    "age": 60.0,
    "months": 12.0,
    "gender": 1,
    "type": 2,
    "asym": 1
}

task_idx = task_mapping["BBP_NORMAL"]

score = predict_from_video_and_audio(
    video_path,
    audio_path,
    metadata,
    task_idx
)

print("Predicted Recovery %:", round(score, 2))

