# -*- coding: utf-8 -*-
"""Multimodal.ipynb

Automatically generated by Colab.

Original file is located at
"""

import random
import numpy as np
import torch

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import mean_absolute_error, r2_score
from scipy.stats import pearsonr
import numpy as np
from collections import defaultdict

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

landmark_data = torch.load(
    "path_to_dataset.pt",
    map_location=device,
    weights_only=False
)

audio_data = torch.load(
    "path_to_audio_features.pt",
    map_location=device, weights_only=False
)

metadata_data = torch.load(
    "path_to_metadata_features.pt",
    map_location=device,  weights_only=False
)

landmark_encoder = torch.load(
    "path_to_landmark_encoder.pt",
    map_location=device,  weights_only=False
)

task_encoder = torch.load(
    "path_to_task_encoder.pt",
    map_location=device,  weights_only=False
)

landmarks = landmark_data["landmarks"]
labels = landmark_data["labels"]
lm_subjects = landmark_data["subjects"]
lm_tasks = landmark_data["tasks"]

audio_features = audio_data["features"]
audio_subjects = audio_data["subjects"]
audio_tasks = audio_data["tasks"]

meta_subjects = metadata_data["subjects"]

audio_lookup = {}
for i, (s, t) in enumerate(zip(audio_subjects, audio_tasks)):
    audio_lookup[(s, t)] = audio_features[i]

meta_lookup = {}
for i, s in enumerate(meta_subjects):
    meta_lookup[s] = {
        "age": metadata_data["age"][i],
        "months": metadata_data["months"][i],
        "gender": metadata_data["gender"][i],
        "type": metadata_data["type"][i],
        "asym": metadata_data["asym"][i]
    }

class MultimodalDataset(Dataset):
    def __init__(self):
        self.samples = []

        for i in range(len(landmarks)):
            key = (lm_subjects[i], lm_tasks[i])

            if key in audio_lookup and lm_subjects[i] in meta_lookup:
                self.samples.append({
                    "landmark": landmarks[i],
                    "audio": audio_lookup[key],
                    "label": labels[i],
                    "subject": lm_subjects[i],
                    "task": lm_tasks[i]
                })

        self.subjects = sorted(list(set([s["subject"] for s in self.samples])))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        meta = meta_lookup[s["subject"]]

        return (
            s["landmark"].float(),
            s["audio"].float(),
            torch.stack([meta["age"], meta["months"]]).float(),
            meta["gender"].long(),
            meta["type"].long(),
            meta["asym"].long(),
            torch.tensor(task_to_idx[s["task"]]).long(),
            s["label"].float(),
            s["subject"]
        )

unique_tasks = sorted(list(set(lm_tasks)))
task_to_idx = {t: i for i, t in enumerate(unique_tasks)}

class AttentionPooling(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.attn = nn.Linear(input_dim, 1)

    def forward(self, x):
        weights = torch.softmax(self.attn(x), dim=1)
        return torch.sum(weights * x, dim=1)


class LandmarkEncoder(nn.Module):
    def __init__(self):
        super().__init__()

        self.lstm = nn.LSTM(
            input_size=68 * 4,   # MUST BE 272
            hidden_size=96,
            batch_first=True,
            bidirectional=True
        )

        self.attention = AttentionPooling(192)
        self.norm = nn.LayerNorm(192)

    def forward(self, x):
        B, T, P, C = x.shape
        x = x.view(B, T, -1)
        out, _ = self.lstm(x)
        pooled = self.attention(out)
        return self.norm(pooled)

class TaskEncoder(nn.Module):
    def __init__(self, num_tasks=9):
        super().__init__()
        self.embedding = nn.Embedding(num_tasks, 16)
        self.norm = nn.LayerNorm(16)

    def forward(self, x):
        return self.norm(self.embedding(x))

landmark_encoder = LandmarkEncoder().to(device)
task_encoder = TaskEncoder().to(device)

landmark_encoder.load_state_dict(
    torch.load("path_to_landmark_encoder.pt", map_location=device)
)

task_encoder.load_state_dict(
    torch.load("path_to_task_encoder.pt", map_location=device)
)

for p in landmark_encoder.parameters():
    p.requires_grad = False

for p in task_encoder.parameters():
    p.requires_grad = False

class AudioEncoder(nn.Module):
    def __init__(self, input_dim=30, embed_dim=32):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.LayerNorm(64),
            nn.Linear(64, embed_dim),
            nn.LayerNorm(embed_dim)
        )

    def forward(self, x):
        return self.model(x)

class MetadataEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.gender_emb = nn.Embedding(2, 4)
        self.type_emb = nn.Embedding(5, 4)
        self.asym_emb = nn.Embedding(5, 4)

        self.cont_proj = nn.Linear(2, 8)

        self.fusion = nn.Sequential(
            nn.Linear(4 + 4 + 4 + 8, 16),
            nn.ReLU(),
            nn.LayerNorm(16)
        )

    def forward(self, cont, gender, typ, asym):
        g = self.gender_emb(gender)
        t = self.type_emb(typ)
        a = self.asym_emb(asym)
        c = self.cont_proj(cont)
        x = torch.cat([g, t, a, c], dim=1)
        return self.fusion(x)

def add_velocity(x):
    # x shape: (B, 30, 68, 2)
    vel = torch.zeros_like(x)
    vel[:, 1:] = x[:, 1:] - x[:, :-1]
    return torch.cat([x, vel], dim=-1)

class MultimodalModel(nn.Module):
    def __init__(self, landmark_encoder, task_encoder):
        super().__init__()

        self.landmark_encoder = landmark_encoder
        self.task_encoder = task_encoder
        self.audio_encoder = AudioEncoder()
        self.meta_encoder = MetadataEncoder()

        self.fusion_head = nn.Sequential(
            nn.Linear(192 + 32 + 16 + 16, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, lm, audio, cont, gender, typ, asym, task_idx):
        lm_emb = self.landmark_encoder(lm)
        task_emb = self.task_encoder(task_idx)
        audio_emb = self.audio_encoder(audio)
        meta_emb = self.meta_encoder(cont, gender, typ, asym)

        x = torch.cat([lm_emb, audio_emb, meta_emb, task_emb], dim=1)
        return self.fusion_head(x)

"""#old one"""

dataset = MultimodalDataset()
subjects = dataset.subjects

all_mae = []
all_r2 = []
all_corr = []

all_preds = []
all_truths = []

for test_subject in subjects:

    train_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] != test_subject]
    test_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] == test_subject]

    train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx), batch_size=8, shuffle=True)
    test_loader = DataLoader(torch.utils.data.Subset(dataset, test_idx), batch_size=8)

    model = MultimodalModel(landmark_encoder, task_encoder)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=1e-3
    )

    # Stage 1
    for epoch in range(40):
        model.train()
        for lm, audio, cont, g, t, a, task_idx, y, _ in train_loader:
            lm = add_velocity(lm)
            pred = model(lm, audio, cont, g, t, a, task_idx)
            loss = criterion(pred.squeeze(), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Stage 2 (unfreeze all)
    for p in model.parameters():
        p.requires_grad = True

    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(20):
        model.train()
        for lm, audio, cont, g, t, a, task_idx, y, _ in train_loader:
            lm = add_velocity(lm)
            pred = model(lm, audio, cont, g, t, a, task_idx)
            loss = criterion(pred.squeeze(), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    preds = []
    truths = []

    with torch.no_grad():
        for lm, audio, cont, g, t, a, task_idx, y, _ in test_loader:
            lm = add_velocity(lm)
            pred = model(lm, audio, cont, g, t, a, task_idx)
            preds.extend(pred.squeeze().numpy())
            truths.extend(y.numpy())

    all_preds.extend(preds)
    all_truths.extend(truths)


    mae = mean_absolute_error(truths, preds)
    r2 = r2_score(truths, preds)
    corr = pearsonr(truths, preds)[0]

    all_mae.append(mae)
    all_r2.append(r2)
    all_corr.append(corr)
    print("Pred mean:", np.mean(preds))
    print("GT mean:", np.mean(truths))
    print("Pred std:", np.std(preds))
    print("GT std:", np.std(truths))


print("Mean MAE:", np.mean(all_mae))
print("Std MAE:", np.std(all_mae))
overall_r2 = r2_score(all_truths, all_preds)
overall_pearson = pearsonr(all_truths, all_preds)[0]

print("Overall R2:", overall_r2)
print("Overall Pearson:", overall_pearson)

seeds = [42, 123, 999]

for seed in seeds:

    print("\nRunning seed:", seed)
    set_seed(seed)

    dataset = MultimodalDataset()
    subjects = dataset.subjects

    all_mae = []
    all_r2 = []
    all_corr = []

    all_preds = []
    all_truths = []

    for test_subject in subjects:

        train_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] != test_subject]
        test_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] == test_subject]

        train_loader = DataLoader(
            torch.utils.data.Subset(dataset, train_idx),
            batch_size=8,
            shuffle=True
        )

        test_loader = DataLoader(
            torch.utils.data.Subset(dataset, test_idx),
            batch_size=8
        )

        model = MultimodalModel(landmark_encoder, task_encoder)

        # Freeze backbone permanently
        for p in model.landmark_encoder.parameters():
            p.requires_grad = False
        for p in model.task_encoder.parameters():
            p.requires_grad = False

        criterion = nn.MSELoss()
        optimizer = optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=1e-3,
            weight_decay=1e-4
        )

        # Single training stage only
        for epoch in range(40):
            model.train()
            for lm, audio, cont, g, t, a, task_idx, y, _ in train_loader:
                lm = add_velocity(lm)
                pred = model(lm, audio, cont, g, t, a, task_idx)
                loss = criterion(pred.squeeze(), y)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        # Evaluation
        model.eval()
        preds = []
        truths = []

        with torch.no_grad():
            for lm, audio, cont, g, t, a, task_idx, y, _ in test_loader:
                lm = add_velocity(lm)
                pred = model(lm, audio, cont, g, t, a, task_idx)
                preds.extend(pred.squeeze().numpy())
                truths.extend(y.numpy())

        all_preds.extend(preds)
        all_truths.extend(truths)

        mae = mean_absolute_error(truths, preds)
        r2 = r2_score(truths, preds)
        corr = pearsonr(truths, preds)[0]

        all_mae.append(mae)
        all_r2.append(r2)
        all_corr.append(corr)

    print("Mean MAE:", np.mean(all_mae))
    print("Std MAE:", np.std(all_mae))

    overall_r2 = r2_score(all_truths, all_preds)
    overall_pearson = pearsonr(all_truths, all_preds)[0]

    print("Overall R2:", overall_r2)
    print("Overall Pearson:", overall_pearson)

torch.save(model.state_dict(), "path_to_save_model.pt")

"""#Explainabilitiy"""

!pip install shap

class AttentionPoolingXAI(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.attn = nn.Linear(input_dim, 1)

    def forward(self, x):
        weights = torch.softmax(self.attn(x), dim=1)
        pooled = torch.sum(weights * x, dim=1)
        return pooled, weights

class LandmarkEncoderXAI(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=68 * 4,
            hidden_size=96,
            batch_first=True,
            bidirectional=True
        )
        self.attention = AttentionPoolingXAI(192)
        self.norm = nn.LayerNorm(192)

    def forward(self, x):
        B, T, P, C = x.shape
        x = x.view(B, T, -1)
        out, _ = self.lstm(x)
        pooled, weights = self.attention(out)
        return self.norm(pooled), weights

class MultimodalModelXAI(nn.Module):
    def __init__(self):
        super().__init__()

        self.landmark_encoder = LandmarkEncoderXAI()
        self.task_encoder = TaskEncoder()
        self.audio_encoder = AudioEncoder()
        self.meta_encoder = MetadataEncoder()

        self.fusion_head = nn.Sequential(
            nn.Linear(192 + 32 + 16 + 16, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, lm, audio, cont, gender, typ, asym, task_idx):

        lm_emb, attn_weights = self.landmark_encoder(lm)
        task_emb = self.task_encoder(task_idx)
        audio_emb = self.audio_encoder(audio)
        meta_emb = self.meta_encoder(cont, gender, typ, asym)

        x = torch.cat([lm_emb, audio_emb, meta_emb, task_emb], dim=1)

        pred = self.fusion_head(x)

        return pred, attn_weights

xai_model = MultimodalModelXAI().to(device)

state_dict = torch.load(
    "path_to_save_model.pt",
    map_location=device
)

xai_model.load_state_dict(state_dict)
xai_model.eval()

from sklearn.metrics import mean_squared_error, accuracy_score, f1_score

dataset = MultimodalDataset()
subjects = dataset.subjects

all_mae = []
all_r2 = []
all_corr = []
all_rmse = []

all_preds = []
all_truths = []

for test_subject in subjects:

    train_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] != test_subject]
    test_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] == test_subject]

    train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx), batch_size=8, shuffle=True)
    test_loader = DataLoader(torch.utils.data.Subset(dataset, test_idx), batch_size=8)

    model = MultimodalModel(landmark_encoder, task_encoder)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=1e-3
    )

    # Stage 1
    for epoch in range(40):
        model.train()
        for lm, audio, cont, g, t, a, task_idx, y, _ in train_loader:
            lm = add_velocity(lm)
            pred, attn = xai_model(lm, audio, cont, g, t, a, task_idx)
            loss = criterion(pred.squeeze(), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Stage 2 (unfreeze all)
    for p in model.parameters():
        p.requires_grad = True

    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(20):
        model.train()
        for lm, audio, cont, g, t, a, task_idx, y, _ in train_loader:
            lm = add_velocity(lm)
            pred, attn = xai_model(lm, audio, cont, g, t, a, task_idx)
            loss = criterion(pred.squeeze(), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    preds = []
    truths = []

    with torch.no_grad():
        for lm, audio, cont, g, t, a, task_idx, y, _ in test_loader:
            lm = add_velocity(lm)
            pred,attn = xai_model(lm, audio, cont, g, t, a, task_idx)
            preds.extend(pred.squeeze().numpy())
            truths.extend(y.numpy())

    all_preds.extend(preds)
    all_truths.extend(truths)


    mae = mean_absolute_error(truths, preds)
    r2 = r2_score(truths, preds)
    corr = pearsonr(truths, preds)[0]
    rmse = np.sqrt(mean_squared_error(truths, preds))

    all_mae.append(mae)
    all_r2.append(r2)
    all_corr.append(corr)
    all_rmse.append(rmse)

    print("Pred mean:", np.mean(preds))
    print("GT mean:", np.mean(truths))
    print("Pred std:", np.std(preds))
    print("GT std:", np.std(truths))


print("Mean MAE:", np.mean(all_mae))
print("Std MAE:", np.std(all_mae))
overall_r2 = r2_score(all_truths, all_preds)
overall_pearson = pearsonr(all_truths, all_preds)[0]

print("Overall R2:", overall_r2)
print("Overall Pearson:", overall_pearson)
print("Mean RMSE:", np.mean(all_rmse))
print("Std RMSE:", np.std(all_rmse))

import numpy as np
import matplotlib.pyplot as plt

frames = np.arange(len(weights))

peak_idx = np.argmax(weights)

plt.figure(figsize=(10,5))
plt.plot(frames, weights, linewidth=2)
plt.scatter(peak_idx, weights[peak_idx], color='red', zorder=5)

plt.title("Temporal Attention Over Frames")
plt.xlabel("Frame Index (0–29)")
plt.ylabel("Attention Weight")
plt.grid(alpha=0.3)

plt.annotate(
    f"Peak Frame: {peak_idx}",
    (peak_idx, weights[peak_idx]),
    textcoords="offset points",
    xytext=(0,10),
    ha='center'
)

plt.show()

print("Peak frame index:", peak_idx)
print("Max attention weight:", weights[peak_idx])
print("Sum of weights:", weights.sum())

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_infer = MultimodalModel(landmark_encoder, task_encoder).to(device)

state_dict = torch.load(
    "path_to_save_model.pt",
    map_location=device
)

model_infer.load_state_dict(state_dict)
model_infer.eval()

"""#Modality"""

for lm, audio, cont, g, t, a, task_idx, y, _ in test_loader:
    lm = add_velocity(lm)

    lm_1 = lm[0:1]
    audio_1 = audio[0:1]
    cont_1 = cont[0:1]
    g_1 = g[0:1]
    t_1 = t[0:1]
    a_1 = a[0:1]
    task_1 = task_idx[0:1]
    break

with torch.no_grad():
    full_pred, _ = xai_model(lm_1, audio_1, cont_1, g_1, t_1, a_1, task_1)

full_val = full_pred.item()

zero_audio = torch.zeros_like(audio_1)

with torch.no_grad():
    pred_no_audio, _ = xai_model(lm_1, zero_audio, cont_1, g_1, t_1, a_1, task_1)

audio_effect = full_val - pred_no_audio.item()

zero_lm = torch.zeros_like(lm_1)

with torch.no_grad():
    pred_no_lm, _ = xai_model(zero_lm, audio_1, cont_1, g_1, t_1, a_1, task_1)

landmark_effect = full_val - pred_no_lm.item()

zero_cont = torch.zeros_like(cont_1)

with torch.no_grad():
    pred_no_meta, _ = xai_model(lm_1, audio_1, zero_cont, g_1, t_1, a_1, task_1)

meta_effect = full_val - pred_no_meta.item()

abs_audio = abs(audio_effect)
abs_landmark = abs(landmark_effect)
abs_meta = abs(meta_effect)

total = abs_audio + abs_landmark + abs_meta

audio_pct = 100 * abs_audio / total
landmark_pct = 100 * abs_landmark / total
meta_pct = 100 * abs_meta / total

print("Prediction (0–100):", round(full_val, 2))
print("")
print("Modality Contribution:")
print(f"Landmark: {landmark_pct:.2f}%")
print(f"Audio: {audio_pct:.2f}%")
print(f"Metadata: {meta_pct:.2f}%")

import matplotlib.pyplot as plt
import numpy as np

modalities = ["Landmark", "Audio", "Metadata"]
percentages = [landmark_pct, audio_pct, meta_pct]

plt.figure(figsize=(6,5))
bars = plt.bar(modalities, percentages)

plt.ylabel("Contribution (%)")
plt.title("Modality Contribution to Prediction")

# Add percentage labels above bars
for i, v in enumerate(percentages):
    plt.text(i, v + 1, f"{v:.1f}%", ha='center')

plt.ylim(0, 100)
plt.show()

"""#Visulaizations

Frame With Landmarks Overlay
"""

import cv2
import numpy as np

def sample_frames(video_path, num_frames=30):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if total_frames == 0:
        cap.release()
        return None

    indices = np.linspace(0, total_frames - 1, num_frames).astype(int)

    frames = []
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            cap.release()
            return None
        frames.append(frame)

    cap.release()
    return frames

import pandas as pd

txt_path = "path_to_txt_file.txt"  # change path if needed
df = pd.read_csv(txt_path)

print(df.head())

total_frames = len(df)

sampled_indices = np.linspace(
    0,
    total_frames - 1,
    30
).astype(int)

print("Total frames in txt:", total_frames)
print("Sampled indices:", sampled_indices)

true_index = sampled_indices[peak_idx]

frame_number = df.iloc[true_index]["Frame"]

print("Peak sampled index:", peak_idx)
print("Mapped true index:", true_index)
print("Actual frame number:", frame_number)

row = df[df["Frame"] == frame_number].iloc[0]

coords = row.values[1:]          # skip frame column
coords = np.array(coords).reshape(68, 2)

video_path = "path_to_video.avi"

import cv2

frame_number = int(frame_number)  # ensure integer

cap = cv2.VideoCapture(video_path)

total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print("Total frames in video:", total_video_frames)

if frame_number >= total_video_frames:
    print("Frame number exceeds video length")
else:
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
    ret, frame = cap.read()
    print("Frame read status:", ret)

cap.release()

import matplotlib.pyplot as plt

plt.figure(figsize=(6,6))
plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

for (x, y) in coords:
    plt.scatter(x, y, s=10)

plt.title(f"Frame {frame_number}")
plt.axis("off")
plt.show()

"""Vector motion"""

baseline_index = sampled_indices[0]

baseline_frame_number = int(df.iloc[baseline_index]["Frame"])
peak_frame_number = int(df.iloc[true_index]["Frame"])

print("Baseline frame:", baseline_frame_number)
print("Peak frame:", peak_frame_number)

# Baseline landmarks
row_base = df[df["Frame"] == baseline_frame_number].iloc[0]
coords_base = np.array(row_base.values[1:]).reshape(68, 2)

# Peak landmarks
row_peak = df[df["Frame"] == peak_frame_number].iloc[0]
coords_peak = np.array(row_peak.values[1:]).reshape(68, 2)

dx = coords_peak[:, 0] - coords_base[:, 0]
dy = coords_peak[:, 1] - coords_base[:, 1]

import cv2

cap = cv2.VideoCapture(video_path)
cap.set(cv2.CAP_PROP_POS_FRAMES, baseline_frame_number - 1)
ret, frame_base = cap.read()
cap.release()

print("Frame read success:", ret)

import matplotlib.pyplot as plt

plt.figure(figsize=(6,6))
plt.imshow(cv2.cvtColor(frame_base, cv2.COLOR_BGR2RGB))

# Plot baseline points
plt.scatter(coords_base[:,0], coords_base[:,1], s=10)

# Draw arrows
for i in range(68):
    plt.arrow(
        coords_base[i,0],
        coords_base[i,1],
        dx[i],
        dy[i],
        head_width=2,
        length_includes_head=True
    )

plt.title("Landmark Motion: Baseline → Peak Attention")
plt.axis("off")
plt.show()

"""Regression Fit Plot"""

import numpy as np

preds = np.array(all_preds).flatten()
truths = np.array(all_truths).flatten()

print("Pred shape:", preds.shape)
print("Truth shape:", truths.shape)

from sklearn.metrics import r2_score, mean_squared_error
from scipy.stats import pearsonr

r2 = r2_score(truths, preds)
pearson_corr, _ = pearsonr(truths, preds)
rmse = np.sqrt(mean_squared_error(truths, preds))
mae = np.mean(np.abs(preds - truths))

print("R²:", r2)
print("Pearson:", pearson_corr)
print("RMSE:", rmse)
print("MAE:", mae)

import matplotlib.pyplot as plt

plt.figure(figsize=(6,6))

plt.scatter(truths, preds)

# Identity line y = x
min_val = min(truths.min(), preds.min())
max_val = max(truths.max(), preds.max())
plt.plot([min_val, max_val], [min_val, max_val])

plt.xlabel("Ground Truth Recovery (%)")
plt.ylabel("Predicted Recovery (%)")
plt.title("Regression Fit: Predicted vs Ground Truth")

# Add metrics inside plot
plt.text(
    min_val,
    max_val,
    f"R² = {r2:.3f}\nPearson = {pearson_corr:.3f}\nRMSE = {rmse:.2f}\nMAE = {mae:.2f}",
    verticalalignment='top'
)

plt.show()

"""Error Distribution Histogram"""

import numpy as np

preds = np.array(all_preds).flatten()
truths = np.array(all_truths).flatten()

errors = preds - truths

mean_error = np.mean(errors)
std_error = np.std(errors)

print("Mean Error:", mean_error)
print("Std Error:", std_error)

import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))

plt.hist(errors, bins=20)

plt.axvline(0)
plt.axvline(mean_error)

plt.xlabel("Prediction Error (Pred - Ground Truth)")
plt.ylabel("Frequency")
plt.title("Error Distribution Histogram")

plt.text(
    min(errors),
    max(plt.gca().get_ylim()),
    f"Mean = {mean_error:.2f}\nStd = {std_error:.2f}",
    verticalalignment='top'
)

plt.show()

