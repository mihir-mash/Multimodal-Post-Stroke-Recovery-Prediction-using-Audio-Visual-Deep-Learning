# -*- coding: utf-8 -*-
"""Video_only pipeline.ipynb

Automatically generated by Colab.

Original file is located at
"""

pip install mediapipe==0.10.32 opencv-python moviepy librosa joblib

!pip uninstall mediapipe -y
!pip install mediapipe==0.10.14

import cv2
import numpy as np
import torch
import torch.nn as nn
import mediapipe as mp

mp_face = mp.solutions.face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1
)

def extract_frames(video_path, num_frames=30):
    cap = cv2.VideoCapture(video_path)
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    indices = np.linspace(0, total - 1, num_frames).astype(int)

    frames = []
    for i in range(total):
        ret, frame = cap.read()
        if not ret:
            break
        if i in indices:
            frames.append(frame)

    cap.release()
    return frames


def get_landmarks(frame):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = mp_face.process(rgb)

    if not results.multi_face_landmarks:
        return None

    h, w = frame.shape[:2]
    lm = results.multi_face_landmarks[0].landmark
    coords = np.array([[p.x * w, p.y * h] for p in lm[:68]])
    return coords


def normalize_landmarks(lm, width, height):
    lm = lm.astype(np.float32)
    lm[:, 0] /= width
    lm[:, 1] /= height
    centroid = np.mean(lm, axis=0)
    lm -= centroid
    return lm


def add_velocity(x):
    vel = torch.zeros_like(x)
    vel[:, 1:] = x[:, 1:] - x[:, :-1]
    return torch.cat([x, vel], dim=-1)

class MultimodalNoAudioModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.landmark_encoder = LandmarkEncoder()
        self.task_encoder = TaskEncoder()
        self.meta_encoder = MetadataEncoder()

        self.fusion_head = nn.Sequential(
            nn.Linear(192 + 16 + 16, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, lm, cont, gender, typ, asym, task_idx):
        lm_emb = self.landmark_encoder(lm)
        task_emb = self.task_encoder(task_idx)
        meta_emb = self.meta_encoder(cont, gender, typ, asym)

        x = torch.cat([lm_emb, meta_emb, task_emb], dim=1)
        return self.fusion_head(x)

class MetadataEncoder(nn.Module):
    def __init__(self):
        super().__init__()

        self.gender_emb = nn.Embedding(2, 4)
        self.type_emb = nn.Embedding(5, 4)
        self.asym_emb = nn.Embedding(5, 4)

        self.cont_proj = nn.Linear(2, 8)

        self.fusion = nn.Sequential(
            nn.Linear(4 + 4 + 4 + 8, 16),
            nn.ReLU(),
            nn.LayerNorm(16)
        )

    def forward(self, cont, gender, typ, asym):
        g = self.gender_emb(gender)
        t = self.type_emb(typ)
        a = self.asym_emb(asym)
        c = self.cont_proj(cont)

        x = torch.cat([g, t, a, c], dim=1)
        return self.fusion(x)

device = torch.device("cpu")

model = MultimodalNoAudioModel().to(device)

model.load_state_dict(
    torch.load("path_to_saved wieghts_file-.pt", map_location=device)
)

model.eval()

metadata = {
    "age": 62.0,
    "months": 14.0,
    "gender": 1,     # same encoding used in training
    "type": 2,       # same encoding as dataset
    "asym": 1        # same encoding
}

def predict_from_video(video_path, metadata, task_idx):
    frames = extract_frames(video_path, 30)

    landmark_seq = []

    for frame in frames:
        h, w = frame.shape[:2]
        lm = get_landmarks(frame)
        if lm is None:
            continue

        lm = normalize_landmarks(lm, w, h)
        landmark_seq.append(lm)

    if len(landmark_seq) < 30:
        raise ValueError("Face not detected in enough frames")

    landmark_seq = np.stack(landmark_seq[:30])

    lm_tensor = torch.tensor(landmark_seq).unsqueeze(0).float()
    lm_tensor = add_velocity(lm_tensor)

    cont = torch.tensor([[metadata["age"], metadata["months"]]]).float()
    gender = torch.tensor([metadata["gender"]]).long()
    typ = torch.tensor([metadata["type"]]).long()
    asym = torch.tensor([metadata["asym"]]).long()
    task_tensor = torch.tensor([task_idx]).long()

    with torch.no_grad():
        pred = model(lm_tensor, cont, gender, typ, asym, task_tensor)

    return float(pred.item())

task_mapping = {
    "BBP_NORMAL": 0,
    "DDK_PA": 1,
    "DDK_PATAKA": 2,
    "NSM_BIGSMILE": 3,
    "NSM_BROW": 4,
    "NSM_KISS": 5,
    "NSM_OPEN": 6,
    "NSM_SPREAD": 7,
    "NSM_BLOW": 8
}

score = predict_from_video(
    "video_path_without_audio",
    metadata,
    task_mapping["NSM_BIGSMILE"]
)

print("Predicted Recovery %:", score)

"""#old"""

frames = extract_frames("patient_video.mp4")

landmark_seq = []

for frame in frames:
    h, w = frame.shape[:2]
    lm = get_landmarks(frame)
    if lm is None:
        continue
    lm = normalize_landmarks(lm, w, h)
    landmark_seq.append(lm)

landmark_seq = np.stack(landmark_seq)  # (30,68,2)

import torch
import torch.nn as nn
import numpy as np

class AttentionPooling(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.attn = nn.Linear(input_dim, 1)

    def forward(self, x):
        weights = torch.softmax(self.attn(x), dim=1)
        return torch.sum(weights * x, dim=1)

class LandmarkEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=68 * 4,
            hidden_size=96,
            batch_first=True,
            bidirectional=True
        )
        self.attention = AttentionPooling(192)
        self.norm = nn.LayerNorm(192)

    def forward(self, x):
        B, T, P, C = x.shape
        x = x.view(B, T, -1)
        out, _ = self.lstm(x)
        pooled = self.attention(out)
        return self.norm(pooled)

class TaskEncoder(nn.Module):
    def __init__(self, num_tasks=9):
        super().__init__()
        self.embedding = nn.Embedding(num_tasks, 16)
        self.norm = nn.LayerNorm(16)

    def forward(self, x):
        return self.norm(self.embedding(x))

class AudioEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(30, 64),
            nn.ReLU(),
            nn.LayerNorm(64),
            nn.Linear(64, 32),
            nn.LayerNorm(32)
        )

    def forward(self, x):
        return self.model(x)

class MetadataEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.gender_emb = nn.Embedding(2, 4)
        self.type_emb = nn.Embedding(5, 4)
        self.asym_emb = nn.Embedding(5, 4)
        self.cont_proj = nn.Linear(2, 8)
        self.fusion = nn.Sequential(
            nn.Linear(20, 16),
            nn.ReLU(),
            nn.LayerNorm(16)
        )

    def forward(self, cont, gender, typ, asym):
        g = self.gender_emb(gender)
        t = self.type_emb(typ)
        a = self.asym_emb(asym)
        c = self.cont_proj(cont)
        x = torch.cat([g, t, a, c], dim=1)
        return self.fusion(x)

class MultimodalModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.landmark_encoder = LandmarkEncoder()
        self.task_encoder = TaskEncoder()
        self.audio_encoder = AudioEncoder()
        self.meta_encoder = MetadataEncoder()
        self.fusion_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, lm, audio, cont, gender, typ, asym, task_idx):
        lm_emb = self.landmark_encoder(lm)
        task_emb = self.task_encoder(task_idx)
        audio_emb = self.audio_encoder(audio)
        meta_emb = self.meta_encoder(cont, gender, typ, asym)
        x = torch.cat([lm_emb, audio_emb, meta_emb, task_emb], dim=1)
        return self.fusion_head(x)

def add_velocity(x):
    vel = torch.zeros_like(x)
    vel[:, 1:] = x[:, 1:] - x[:, :-1]
    return torch.cat([x, vel], dim=-1)

def predict_recovery(landmarks, audio_features, metadata, task_idx):

    lm = torch.tensor(landmarks).unsqueeze(0).float()
    lm = add_velocity(lm)

    audio = torch.tensor(audio_features).unsqueeze(0).float()

    cont = torch.tensor([[metadata["age"], metadata["months"]]]).float()
    gender = torch.tensor([metadata["gender"]]).long()
    typ = torch.tensor([metadata["type"]]).long()
    asym = torch.tensor([metadata["asym"]]).long()

    task = torch.tensor([task_idx]).long()

    with torch.no_grad():
        pred = model(lm, audio, cont, gender, typ, asym, task)

    return float(pred.item())

from google.colab import files
uploaded = files.upload()

import cv2
import numpy as np

def extract_frames(video_path, num_frames=30):
    cap = cv2.VideoCapture(video_path)
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    indices = np.linspace(0, total-1, num_frames).astype(int)

    frames = []
    for i in range(total):
        ret, frame = cap.read()
        if not ret:
            break
        if i in indices:
            frames.append(frame)

    cap.release()
    return frames

!pip install mediapipe

import mediapipe as mp

mp_face = mp.solutions.face_mesh.FaceMesh(static_image_mode=True)

def get_landmarks(frame):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = mp_face.process(rgb)

    if not results.multi_face_landmarks:
        return None

    h, w = frame.shape[:2]
    lm = results.multi_face_landmarks[0].landmark
    coords = np.array([[p.x*w, p.y*h] for p in lm[:68]])
    return coords

def normalize_landmarks(lm, width, height):
    lm = lm.astype(np.float32)
    lm[:,0] /= width
    lm[:,1] /= height
    centroid = np.mean(lm, axis=0)
    lm -= centroid
    return lm

import moviepy.editor as mp

def extract_audio_from_video(video_path, output_wav="temp.wav"):
    video = mp.VideoFileClip(video_path)
    video.audio.write_audiofile(output_wav, fps=16000)
    return output_wav

import joblib

scaler = joblib.load("audio_scaler.save")

y, sr = preprocess_audio("temp.wav")
audio_features = extract_audio_features(y, sr)

audio_features_scaled = scaler.transform(audio_features.reshape(1,-1))