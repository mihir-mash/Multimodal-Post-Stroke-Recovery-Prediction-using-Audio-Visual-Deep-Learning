# -*- coding: utf-8 -*-
"""VidMultimodal.ipynb

Automatically generated by Colab.

Original file is located at

"""

import random
import numpy as np
import torch

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import mean_absolute_error, r2_score
from scipy.stats import pearsonr
import numpy as np
from collections import defaultdict

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

landmark_data = torch.load(
    "path_to_dataset.pt",
    map_location=device,
    weights_only=False
)

metadata_data = torch.load(
    "path_to_metadata.pt",
    map_location=device,  weights_only=False
)

landmark_encoder = torch.load(
    "path_to_landmark_encoder.pt",
    map_location=device,  weights_only=False
)

task_encoder = torch.load(
    "path_to_task_encoder.pt",
    map_location=device,  weights_only=False
)

landmarks = landmark_data["landmarks"]
labels = landmark_data["labels"]
lm_subjects = landmark_data["subjects"]
lm_tasks = landmark_data["tasks"]


meta_subjects = metadata_data["subjects"]

meta_lookup = {}
for i, s in enumerate(meta_subjects):
    meta_lookup[s] = {
        "age": metadata_data["age"][i],
        "months": metadata_data["months"][i],
        "gender": metadata_data["gender"][i],
        "type": metadata_data["type"][i],
        "asym": metadata_data["asym"][i]
    }

class MultimodalDataset(Dataset):
    def __init__(self):
        self.samples = []

        for i in range(len(landmarks)):
            if lm_subjects[i] in meta_lookup:
                self.samples.append({
                    "landmark": landmarks[i],
                    "label": labels[i],
                    "subject": lm_subjects[i],
                    "task": lm_tasks[i]
                })

        self.subjects = sorted(list(set([s["subject"] for s in self.samples])))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        meta = meta_lookup[s["subject"]]

        return (
            s["landmark"].float(),
            torch.stack([meta["age"], meta["months"]]).float(),
            meta["gender"].long(),
            meta["type"].long(),
            meta["asym"].long(),
            torch.tensor(task_to_idx[s["task"]]).long(),
            s["label"].float(),
            s["subject"]
        )

unique_tasks = sorted(list(set(lm_tasks)))
task_to_idx = {t: i for i, t in enumerate(unique_tasks)}

class AttentionPooling(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.attn = nn.Linear(input_dim, 1)

    def forward(self, x):
        weights = torch.softmax(self.attn(x), dim=1)
        return torch.sum(weights * x, dim=1)


class LandmarkEncoder(nn.Module):
    def __init__(self):
        super().__init__()

        self.lstm = nn.LSTM(
            input_size=68 * 4,   # MUST BE 272
            hidden_size=96,
            batch_first=True,
            bidirectional=True
        )

        self.attention = AttentionPooling(192)
        self.norm = nn.LayerNorm(192)

    def forward(self, x):
        B, T, P, C = x.shape
        x = x.view(B, T, -1)
        out, _ = self.lstm(x)
        pooled = self.attention(out)
        return self.norm(pooled)

class TaskEncoder(nn.Module):
    def __init__(self, num_tasks=9):
        super().__init__()
        self.embedding = nn.Embedding(num_tasks, 16)
        self.norm = nn.LayerNorm(16)

    def forward(self, x):
        return self.norm(self.embedding(x))

landmark_encoder = LandmarkEncoder().to(device)
task_encoder = TaskEncoder().to(device)

landmark_encoder.load_state_dict(
    torch.load("path_to_landmark_encoder.pt", map_location=device)
)

task_encoder.load_state_dict(
    torch.load("path_to_task_encoder.pt", map_location=device)
)

for p in landmark_encoder.parameters():
    p.requires_grad = False

for p in task_encoder.parameters():
    p.requires_grad = False

class MetadataEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.gender_emb = nn.Embedding(2, 4)
        self.type_emb = nn.Embedding(5, 4)
        self.asym_emb = nn.Embedding(5, 4)

        self.cont_proj = nn.Linear(2, 8)

        self.fusion = nn.Sequential(
            nn.Linear(4 + 4 + 4 + 8, 16),
            nn.ReLU(),
            nn.LayerNorm(16)
        )

    def forward(self, cont, gender, typ, asym):
        g = self.gender_emb(gender)
        t = self.type_emb(typ)
        a = self.asym_emb(asym)
        c = self.cont_proj(cont)
        x = torch.cat([g, t, a, c], dim=1)
        return self.fusion(x)

def add_velocity(x):
    # x shape: (B, 30, 68, 2)
    vel = torch.zeros_like(x)
    vel[:, 1:] = x[:, 1:] - x[:, :-1]
    return torch.cat([x, vel], dim=-1)

class MultimodalModel(nn.Module):
    def __init__(self, landmark_encoder, task_encoder):
        super().__init__()

        self.landmark_encoder = landmark_encoder
        self.task_encoder = task_encoder
        self.meta_encoder = MetadataEncoder()

        self.fusion_head = nn.Sequential(
            nn.Linear(192 + 16 + 16, 128),  # 192 landmark + 16 meta + 16 task
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, lm, cont, gender, typ, asym, task_idx):
        lm_emb = self.landmark_encoder(lm)
        task_emb = self.task_encoder(task_idx)
        meta_emb = self.meta_encoder(cont, gender, typ, asym)

        x = torch.cat([lm_emb, meta_emb, task_emb], dim=1)
        return self.fusion_head(x)

dataset = MultimodalDataset()
subjects = dataset.subjects

all_mae = []
all_r2 = []
all_corr = []

all_preds = []
all_truths = []

for test_subject in subjects:

    train_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] != test_subject]
    test_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] == test_subject]

    train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx), batch_size=8, shuffle=True)
    test_loader = DataLoader(torch.utils.data.Subset(dataset, test_idx), batch_size=8)

    model = MultimodalModel(landmark_encoder, task_encoder)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=1e-3
    )

    # Stage 1
    for epoch in range(40):
        model.train()
        for lm, cont, g, t, a, task_idx, y, _ in train_loader:
            lm = add_velocity(lm)
            pred = model(lm, cont, g, t, a, task_idx)
            loss = criterion(pred.squeeze(), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Stage 2 (unfreeze all)
    for p in model.parameters():
        p.requires_grad = True

    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(20):
        model.train()
        for lm, cont, g, t, a, task_idx, y, _ in train_loader:
            lm = add_velocity(lm)
            pred = model(lm, cont, g, t, a, task_idx)
            loss = criterion(pred.squeeze(), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    preds = []
    truths = []

    with torch.no_grad():
        for lm, cont, g, t, a, task_idx, y, _ in test_loader:
            lm = add_velocity(lm)
            pred = model(lm, cont, g, t, a, task_idx)
            preds.extend(pred.squeeze().numpy())
            truths.extend(y.numpy())

    all_preds.extend(preds)
    all_truths.extend(truths)


    mae = mean_absolute_error(truths, preds)
    r2 = r2_score(truths, preds)
    corr = pearsonr(truths, preds)[0]

    all_mae.append(mae)
    all_r2.append(r2)
    all_corr.append(corr)
    print("Pred mean:", np.mean(preds))
    print("GT mean:", np.mean(truths))
    print("Pred std:", np.std(preds))
    print("GT std:", np.std(truths))


print("Mean MAE:", np.mean(all_mae))
print("Std MAE:", np.std(all_mae))
overall_r2 = r2_score(all_truths, all_preds)
overall_pearson = pearsonr(all_truths, all_preds)[0]

print("Overall R2:", overall_r2)
print("Overall Pearson:", overall_pearson)

seeds = [42, 123, 999]

for seed in seeds:

    print("\nRunning seed:", seed)
    set_seed(seed)

    dataset = MultimodalDataset()
    subjects = dataset.subjects

    all_mae = []
    all_r2 = []
    all_corr = []

    all_preds = []
    all_truths = []

    for test_subject in subjects:

        train_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] != test_subject]
        test_idx = [i for i, s in enumerate(dataset.samples) if s["subject"] == test_subject]

        train_loader = DataLoader(
            torch.utils.data.Subset(dataset, train_idx),
            batch_size=8,
            shuffle=True
        )

        test_loader = DataLoader(
            torch.utils.data.Subset(dataset, test_idx),
            batch_size=8
        )

        model = MultimodalModel(landmark_encoder, task_encoder)

        # Freeze backbone permanently
        for p in model.landmark_encoder.parameters():
            p.requires_grad = False
        for p in model.task_encoder.parameters():
            p.requires_grad = False

        criterion = nn.MSELoss()
        optimizer = optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=1e-3,
            weight_decay=1e-4
        )

        # Single training stage only
        for epoch in range(40):
            model.train()
            for lm, audio, cont, g, t, a, task_idx, y, _ in train_loader:
                lm = add_velocity(lm)
                pred = model(lm, audio, cont, g, t, a, task_idx)
                loss = criterion(pred.squeeze(), y)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        # Evaluation
        model.eval()
        preds = []
        truths = []

        with torch.no_grad():
            for lm, audio, cont, g, t, a, task_idx, y, _ in test_loader:
                lm = add_velocity(lm)
                pred = model(lm, audio, cont, g, t, a, task_idx)
                preds.extend(pred.squeeze().numpy())
                truths.extend(y.numpy())

        all_preds.extend(preds)
        all_truths.extend(truths)

        mae = mean_absolute_error(truths, preds)
        r2 = r2_score(truths, preds)
        corr = pearsonr(truths, preds)[0]

        all_mae.append(mae)
        all_r2.append(r2)
        all_corr.append(corr)

    print("Mean MAE:", np.mean(all_mae))
    print("Std MAE:", np.std(all_mae))

    overall_r2 = r2_score(all_truths, all_preds)
    overall_pearson = pearsonr(all_truths, all_preds)[0]

    print("Overall R2:", overall_r2)
    print("Overall Pearson:", overall_pearson)

torch.save(model.state_dict(), "path_to_save_model.pt")

